{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063f8561",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Sommaire<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Exploration-des-données\" data-toc-modified-id=\"Exploration-des-données-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Exploration des données</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_exploration()\" data-toc-modified-id=\"my_exploration()-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>my_exploration()</a></span></li><li><span><a href=\"#my_valeurs_manquantes()\" data-toc-modified-id=\"my_valeurs_manquantes()-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>my_valeurs_manquantes()</a></span></li><li><span><a href=\"#my_valeurs_manquantes_lignes()\" data-toc-modified-id=\"my_valeurs_manquantes_lignes()-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>my_valeurs_manquantes_lignes()</a></span></li><li><span><a href=\"#my_valeurs_manquantes_colonnes()\" data-toc-modified-id=\"my_valeurs_manquantes_colonnes()-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>my_valeurs_manquantes_colonnes()</a></span></li><li><span><a href=\"#my_first_analyse()\" data-toc-modified-id=\"my_first_analyse()-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>my_first_analyse()</a></span></li><li><span><a href=\"#my_data_visualisation()\" data-toc-modified-id=\"my_data_visualisation()-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>my_data_visualisation()</a></span></li></ul></li><li><span><a href=\"#Nettoyage-des-données\" data-toc-modified-id=\"Nettoyage-des-données-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Nettoyage des données</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_missing_values()\" data-toc-modified-id=\"my_missing_values()-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>my_missing_values()</a></span></li><li><span><a href=\"#my_fill_missing_data()\" data-toc-modified-id=\"my_fill_missing_data()-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>my_fill_missing_data()</a></span></li></ul></li><li><span><a href=\"#Transformation-des-données\" data-toc-modified-id=\"Transformation-des-données-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Transformation des données</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_transformation_df()\" data-toc-modified-id=\"my_transformation_df()-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>my_transformation_df()</a></span></li><li><span><a href=\"#my_data_scaler()\" data-toc-modified-id=\"my_data_scaler()-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>my_data_scaler()</a></span></li><li><span><a href=\"#My_inverse_log\" data-toc-modified-id=\"My_inverse_log-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>My_inverse_log</a></span></li></ul></li><li><span><a href=\"#Analyse-Exploratoire-de-Données\" data-toc-modified-id=\"Analyse-Exploratoire-de-Données-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Analyse Exploratoire de Données</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analyse-Univériée\" data-toc-modified-id=\"Analyse-Univériée-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Analyse Univériée</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_analyse_univarie()\" data-toc-modified-id=\"my_analyse_univarie()-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>my_analyse_univarie()</a></span></li><li><span><a href=\"#my_boxplots()\" data-toc-modified-id=\"my_boxplots()-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>my_boxplots()</a></span></li><li><span><a href=\"#my_outliers_zscore()\" data-toc-modified-id=\"my_outliers_zscore()-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>my_outliers_zscore()</a></span></li><li><span><a href=\"#my_outliers_remover()\" data-toc-modified-id=\"my_outliers_remover()-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>my_outliers_remover()</a></span></li></ul></li><li><span><a href=\"#my_histo_plot()\" data-toc-modified-id=\"my_histo_plot()-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>my_histo_plot()</a></span></li><li><span><a href=\"#Analyse-Bivariée\" data-toc-modified-id=\"Analyse-Bivariée-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Analyse Bivariée</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_corr_heatmap()\" data-toc-modified-id=\"my_corr_heatmap()-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>my_corr_heatmap()</a></span></li><li><span><a href=\"#my_diagramme_violon()\" data-toc-modified-id=\"my_diagramme_violon()-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>my_diagramme_violon()</a></span></li><li><span><a href=\"#my_diagramme_barplot()\" data-toc-modified-id=\"my_diagramme_barplot()-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>my_diagramme_barplot()</a></span></li></ul></li><li><span><a href=\"#Analyse-Multivariée\" data-toc-modified-id=\"Analyse-Multivariée-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Analyse Multivariée</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_anova_test()\" data-toc-modified-id=\"my_anova_test()-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>my_anova_test()</a></span></li><li><span><a href=\"#my_kruskal_wallis_test()\" data-toc-modified-id=\"my_kruskal_wallis_test()-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>my_kruskal_wallis_test()</a></span></li></ul></li><li><span><a href=\"#Tests-statistique\" data-toc-modified-id=\"Tests-statistique-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Tests statistique</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_pearson()\" data-toc-modified-id=\"my_pearson()-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>my_pearson()</a></span></li></ul></li><li><span><a href=\"#Projection-des-individus\" data-toc-modified-id=\"Projection-des-individus-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Projection des individus</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_projection_individus()\" data-toc-modified-id=\"my_projection_individus()-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>my_projection_individus()</a></span></li></ul></li><li><span><a href=\"#Analyse-en-Composante-Principales-(ACP-ou-PCA)\" data-toc-modified-id=\"Analyse-en-Composante-Principales-(ACP-ou-PCA)-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Analyse en Composante Principales (ACP ou PCA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_pca_coude()\" data-toc-modified-id=\"my_pca_coude()-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>my_pca_coude()</a></span></li><li><span><a href=\"#my_pca()\" data-toc-modified-id=\"my_pca()-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>my_pca()</a></span></li></ul></li><li><span><a href=\"#KMeans\" data-toc-modified-id=\"KMeans-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>KMeans</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_kmeans_coude()\" data-toc-modified-id=\"my_kmeans_coude()-4.8.1\"><span class=\"toc-item-num\">4.8.1&nbsp;&nbsp;</span>my_kmeans_coude()</a></span></li><li><span><a href=\"#my_silhouette_method()\" data-toc-modified-id=\"my_silhouette_method()-4.8.2\"><span class=\"toc-item-num\">4.8.2&nbsp;&nbsp;</span>my_silhouette_method()</a></span></li><li><span><a href=\"#my_kmeans()\" data-toc-modified-id=\"my_kmeans()-4.8.3\"><span class=\"toc-item-num\">4.8.3&nbsp;&nbsp;</span>my_kmeans()</a></span></li><li><span><a href=\"#my_all_kmeans()\" data-toc-modified-id=\"my_all_kmeans()-4.8.4\"><span class=\"toc-item-num\">4.8.4&nbsp;&nbsp;</span>my_all_kmeans()</a></span></li></ul></li><li><span><a href=\"#Dendrogramme\" data-toc-modified-id=\"Dendrogramme-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Dendrogramme</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_dendrogram()\" data-toc-modified-id=\"my_dendrogram()-4.9.1\"><span class=\"toc-item-num\">4.9.1&nbsp;&nbsp;</span>my_dendrogram()</a></span></li></ul></li><li><span><a href=\"#PCA-+-Projection-des-individus\" data-toc-modified-id=\"PCA-+-Projection-des-individus-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>PCA + Projection des individus</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_pca_proj()\" data-toc-modified-id=\"my_pca_proj()-4.10.1\"><span class=\"toc-item-num\">4.10.1&nbsp;&nbsp;</span>my_pca_proj()</a></span></li></ul></li></ul></li><li><span><a href=\"#Machine-Learning\" data-toc-modified-id=\"Machine-Learning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#my_backward_selected()\" data-toc-modified-id=\"my_backward_selected()-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>my_backward_selected()</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f6ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib\n",
    "import statsmodels\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "from scipy.stats import f_oneway, shapiro, levene\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing, decomposition\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.compose import make_column_selector, make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c1d5b0",
   "metadata": {},
   "source": [
    "# Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff8eb8",
   "metadata": {},
   "source": [
    "## my_exploration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90a1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_exploration(data):\n",
    "    display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6eb4fd",
   "metadata": {},
   "source": [
    "## my_valeurs_manquantes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f024e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_valeurs_manquantes(data, graphique=True):\n",
    "    #Affichage des % de valeurs manquantes par colonnes.\n",
    "    print(\"Affichage en % des valeurs manquantes par colonnes\")\n",
    "    tableValeurManquante = pd.DataFrame(((data.isna().sum()/data.shape[0])*100).round(2).sort_values(ascending=True))\n",
    "    tableValeurManquante[\"Nb Valeurs Manquantes\"]=data.isna().sum()\n",
    "    display(tableValeurManquante)\n",
    "        #Affichage graphique\n",
    "    if graphique:\n",
    "        plt.figure(figsize=(20,20))\n",
    "        sns.heatmap(data.isna(), cbar=None)\n",
    "        plt.title(\"Représentation graphique des valeurs manquantes\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9651c",
   "metadata": {},
   "source": [
    "## my_valeurs_manquantes_lignes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40922fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_valeurs_manquantes_lignes(df, seuil, method):\n",
    "    \"\"\"\n",
    "    Explication:\n",
    "    Cette fonction recherche les lignes dans un DataFrame qui ont un taux de valeurs manquantes supérieure à un seuil donné en pourcentage.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Le DataFrame dans lequel vous souhaitez rechercher les lignes.\n",
    "    - seuil (float): Le seuil en pourcentage (0-100) utilisé pour filtrer les lignes.\n",
    "    - method (Object) : Si 'inf', récupére les valeurs inférieur au seil, Si \"sup\", récupére les valeurs supérieure au seuil.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - lignes_selectionnees (list): Une liste contenant les indices des lignes qui satisfont au seuil donné.\n",
    "    \"\"\"\n",
    "    lignes_selectionnees = []\n",
    "    nombre_total_lignes = df.shape[0]\n",
    "    \n",
    "    for index, ligne in df.iterrows():\n",
    "        taux_vide_ligne = ligne.isna().sum() / len(ligne)\n",
    "        if method == \"sup\":\n",
    "            if taux_vide_ligne > (seuil / 100):\n",
    "                lignes_selectionnees.append(index)\n",
    "        else:\n",
    "            if taux_vide_ligne < (seuil / 100):\n",
    "                lignes_selectionnees.append(index)\n",
    "    \n",
    "    return lignes_selectionnees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6c8ee",
   "metadata": {},
   "source": [
    "## my_valeurs_manquantes_colonnes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d9374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_valeurs_manquantes_colonnes(df, seuil, method):\n",
    "    \"\"\"\n",
    "    Explication:\n",
    "    Cette fonction recherche les colonnes dans un DataFrame qui ont un taux de valeurs manquantes inférieur à un seuil donné en pourcentage.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Le DataFrame dans lequel vous souhaitez rechercher les colonnes.\n",
    "    - seuil (float): Le seuil en pourcentage (0-100) utilisé pour filtrer les colonnes.\n",
    "    - method (Object) : Si 'inf', récupére les valeurs inférieur au seil, Si \"sup\", récupére les valeurs supérieure au seuil.\n",
    "\n",
    "    Returns:\n",
    "    - colonnes_selectionnees (list): Une liste contenant les noms des colonnes qui satisfont au seuil donné.\n",
    "    \"\"\"\n",
    "    colonnes_selectionnees = []\n",
    "    nombre_total_lignes = df.shape[0]\n",
    "    \n",
    "    for colonne in df.columns:\n",
    "        taux_vide = (df[colonne].isna().sum()) / nombre_total_lignes\n",
    "        if method == \"sup\":\n",
    "            if taux_vide > (seuil / 100):\n",
    "                colonnes_selectionnees.append(colonne)\n",
    "        else:\n",
    "            if taux_vide < (seuil / 100):\n",
    "                colonnes_selectionnees.append(colonne)\n",
    "    \n",
    "    return colonnes_selectionnees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fb0c8",
   "metadata": {},
   "source": [
    "## my_first_analyse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81336b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def my_first_analyse(data, graphique=False):\n",
    "    \"\"\"\n",
    "    Effectue une analyse exploratoire des données et renvoie un résumé statistique.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données à analyser.\n",
    "    Returns:\n",
    "        summary (pandas.DataFrame): DataFrame contenant un résumé statistique des données.\n",
    "    \"\"\"\n",
    "    # Calcul du nombre d'observations et de variables\n",
    "    nb_observations, nb_variables = data.shape\n",
    "    \n",
    "    # Exclure les colonnes non numériques\n",
    "    data_numerique = data.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Calcul des statistiques descriptives uniquement sur les colonnes numériques\n",
    "    moyennes = data_numerique.mean()\n",
    "    medianes = data_numerique.median()\n",
    "    ecart_types = data_numerique.std()\n",
    "    mini = data_numerique.min()\n",
    "    maxi = data_numerique.max()\n",
    "    nb_valeurs_manquantes = data_numerique.isnull().sum()\n",
    "    data_type = data_numerique.dtypes\n",
    "    \n",
    "    # Calcul du nombre de valeurs uniques par colonne\n",
    "    nb_valeurs_uniques = data.nunique()\n",
    "    \n",
    "    # Calcul des 5 premières valeurs les plus fréquentes par colonne\n",
    "    top5_valeurs_frequentes = data.apply(lambda x: ';'.join(x.value_counts().index[:5].astype(str)))\n",
    "    \n",
    "    # Création du DataFrame de résumé statistique\n",
    "    summary = pd.DataFrame({\n",
    "        'observations': nb_observations,\n",
    "        'variables': nb_variables,\n",
    "        'type' : data_type,\n",
    "        'moyennes': moyennes,\n",
    "        'medianes': medianes,\n",
    "        'ecart_types': ecart_types,\n",
    "        'min': mini,\n",
    "        'max': maxi,\n",
    "        'nb_valeurs_manquantes': nb_valeurs_manquantes,\n",
    "        '%_valeurs_manquantes' : round((nb_valeurs_manquantes/nb_observations)*100, 2),\n",
    "        'nb_valeurs_uniques': nb_valeurs_uniques,\n",
    "        'top5_valeurs_frequentes': top5_valeurs_frequentes\n",
    "    })\n",
    "    \n",
    "    if graphique:\n",
    "        # Affichage graphique\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        sns.heatmap(data.isna(), cbar=None)\n",
    "        plt.title(\"Représentation graphique des valeurs manquantes\")\n",
    "        plt.show()\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd2127",
   "metadata": {},
   "source": [
    "## my_data_visualisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d19f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_data_visualisation(df, variable):\n",
    "    \"\"\"\n",
    "    Crée un histogramme de la variable spécifiée.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données à visualiser.\n",
    "        variable (str): Nom de la variable à visualiser.\n",
    "    \"\"\"\n",
    "    df[variable].hist()\n",
    "    plt.title(variable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01cfe4",
   "metadata": {},
   "source": [
    "# Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737aff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09c457ef",
   "metadata": {},
   "source": [
    "## my_missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb917dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_missing_values(df, apply_all=False, strategie=None):\n",
    "    \"\"\"\n",
    "    Traite les données manquantes en fonction de la stratégie spécifiée par l'utilisateur pour chaque colonne.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données à traiter.\n",
    "        apply_all (bool): Si True, applique la même stratégie de traitement pour toutes les colonnes contenant des données manquantes. Sinon, demande à l'utilisateur de choisir une stratégie pour chaque colonne.\n",
    "        strategie : Choix des stratégies : \"valeur_plus_frequente\",\"valeur_constante\",\"supprimer\",\"remplacer_par_moyenne\",\"remplacer_par_mediane\". \n",
    "    Returns:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données traitées.\n",
    "    \"\"\"\n",
    "    strategies = {}\n",
    "    \n",
    "    # Vérifier si toutes les colonnes doivent avoir la même stratégie\n",
    "    if apply_all:\n",
    "        strat = strategie\n",
    "        if strat == 'valeur_plus_frequente':\n",
    "            for col in df.columns[df.isna().any()].tolist():\n",
    "                valeur_plus_frequente = df[col].value_counts().idxmax()\n",
    "                df[col].fillna(valeur_plus_frequente, inplace=True)\n",
    "        elif strat == 'valeur_constante':\n",
    "            constante = float(input(\"Entrez une valeur constante pour remplacer les données manquantes : \"))\n",
    "            for col in df.columns[df.isna().any()].tolist():\n",
    "                df[col].fillna(constante, inplace=True)\n",
    "        elif strat == 'supprimer':\n",
    "            df = df.dropna()\n",
    "        elif strat == 'remplacer_par_moyenne':\n",
    "            df.fillna(df.mean(), inplace=True)\n",
    "        elif strat == 'remplacer_par_mediane':\n",
    "            df.fillna(df.median(), inplace=True)\n",
    "    else:\n",
    "        # Demander à l'utilisateur de choisir une stratégie pour chaque colonne contenant des données manquantes\n",
    "        for col in df.columns[df.isna().any()].tolist():\n",
    "            strat = input(f\"Choisissez une stratégie de traitement pour la colonne '{col}':\\n - 'supprimer' : supprime les lignes contenant des données manquantes\\n - 'remplacer_par_moyenne' : remplace les données manquantes par la moyenne de la colonne\\n - 'remplacer_par_mediane' : remplace les données manquantes par la médiane de la colonne\\n - 'valeur_plus_frequente' : remplace les données manquantes par la valeur la plus fréquente de la colonne\\n - 'valeur_constante' : remplace les données manquantes par une valeur constante\\n\")\n",
    "            strategies[col] = strat\n",
    "\n",
    "        # Appliquer les stratégies de traitement choisies\n",
    "        for col, strat in strategies.items():\n",
    "            if strat == 'supprimer':\n",
    "                df = df.dropna(subset=[col])\n",
    "            elif strat == 'remplacer_par_moyenne':\n",
    "                moyenne = df[col].mean()\n",
    "                df[col].fillna(moyenne, inplace=True)\n",
    "            elif strat == 'remplacer_par_mediane':\n",
    "                mediane = df[col].median()\n",
    "                df[col].fillna(mediane, inplace=True)\n",
    "            elif strat == 'valeur_plus_frequente':\n",
    "                valeur = df[col].mode().iloc[0]\n",
    "                df[col].fillna(valeur, inplace=True)\n",
    "            elif strat == 'valeur_constante':\n",
    "                valeur_constante = input(f\"Entrez une valeur constante pour remplacer les données manquantes de la colonne '{col}': \")\n",
    "                df[col].fillna(valeur_constante, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f94a5",
   "metadata": {},
   "source": [
    "## my_fill_missing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db2e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fill_missing_data(data, method='linear', axis=0):\n",
    "    \"\"\"\n",
    "    Remplit les données manquantes dans un tableau avec la méthode spécifiée.\n",
    "    :param data: Un tableau numpy avec des données manquantes.\n",
    "    :param method: La méthode à utiliser pour remplir les données manquantes.\n",
    "                   Les valeurs acceptées sont : 'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'.\n",
    "                   La valeur par défaut est 'linear'.\n",
    "    :param axis: L'axe pour remplir les données manquantes.\n",
    "                 Si axis = 0 (par défaut), les données manquantes sont remplacées par colonne.\n",
    "                 Si axis = 1, les données manquantes sont remplacées par ligne.\n",
    "    :return: Un tableau numpy avec les données manquantes remplacées.\n",
    "    \"\"\"\n",
    "    # Vérifier que la valeur de l'axe est correcte\n",
    "    if axis != 0 and axis != 1:\n",
    "        raise ValueError(\"L'axe doit être soit 0, soit 1.\")\n",
    "\n",
    "    # Trouver les indices des données manquantes\n",
    "    missing = np.isnan(data)\n",
    "    \n",
    "\n",
    "    # Si toutes les valeurs sont manquantes, retourner le tableau original\n",
    "    if np.all(missing):\n",
    "        return data\n",
    "\n",
    "    # Si aucune valeur n'est manquante, retourner le tableau original\n",
    "    if not np.any(missing):\n",
    "        return data\n",
    "\n",
    "    # Transposer le tableau si l'axe est 1\n",
    "    if axis == 1:\n",
    "        data = data.T\n",
    "        missing = missing.T\n",
    "\n",
    "    # Trouver les indices des données non manquantes\n",
    "    not_missing = np.logical_not(missing)\n",
    "\n",
    "    # Séparer les données manquantes et non manquantes\n",
    "    missing_x = np.argwhere(missing)\n",
    "    missing_y = data[not_missing]\n",
    "    not_missing_x = np.argwhere(not_missing)\n",
    "    not_missing_y = data[not_missing]\n",
    "\n",
    "    # Appliquer la méthode de remplissage\n",
    "    if method == 'linear':\n",
    "        model = LinearRegression()\n",
    "        model.fit(not_missing_x, not_missing_y)\n",
    "        missing_y = model.predict(missing_x)\n",
    "    elif method == 'nearest':\n",
    "        missing_y = np.interp(missing_x, not_missing_x, not_missing_y)\n",
    "    elif method == 'zero':\n",
    "        missing_y = np.zeros(missing_x.shape)\n",
    "    elif method == 'slinear':\n",
    "        missing_y = np.interp(missing_x, not_missing_x, not_missing_y, left=0, right=0)\n",
    "    elif method == 'quadratic':\n",
    "        p = np.polyfit(not_missing_x.flatten(), not_missing_y, 2)\n",
    "        missing_y = np.polyval(p, missing_x.flatten())\n",
    "    elif method == 'cubic':\n",
    "        p = np.polyfit(not_missing_x.flatten(), not_missing_y, 3)\n",
    "        missing_y = np.polyval(p, missing_x.flatten())\n",
    "\n",
    "    # Insérer les données manquantes dans le tableau\n",
    "    data[missing] = missing_y.flatten()\n",
    "\n",
    "    # Transposer le tableau si l'axe est 1\n",
    "    if axis == 1:\n",
    "        data = data.T\n",
    "        \n",
    "    print(missing)\n",
    "    print(data)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2fa7f",
   "metadata": {},
   "source": [
    "# Transformation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef604c",
   "metadata": {},
   "source": [
    "## my_transformation_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a7bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transformation_df(df):\n",
    "    \"\"\"\n",
    "    Applique des transformations de prétraitement à un DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Le DataFrame d'entrée.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame transformé.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Création des sélecteurs de colonnes :\n",
    "    numerical_col = make_column_selector(dtype_include=np.number)\n",
    "    categorical_col = make_column_selector(dtype_exclude=np.number)\n",
    "\n",
    "    # Création du preprocessor :\n",
    "    preprocessor = make_column_transformer((StandardScaler(), numerical_col))\n",
    "    \n",
    "    # Récupération des noms de colonnes numériques et index :\n",
    "    numerical_col_name = list(numerical_col(df))\n",
    "    index = df.index\n",
    "    \n",
    "    # Vérification de la présence de variables catégorielles :\n",
    "    if categorical_col(df):\n",
    "        preprocessor = make_column_transformer(\n",
    "            (StandardScaler(), numerical_col),\n",
    "            (OneHotEncoder(), categorical_col)\n",
    "        )\n",
    "        \n",
    "        # Application de la transformation :\n",
    "        df = preprocessor.fit_transform(df)\n",
    "        \n",
    "        # Récupération des noms de colonnes catégorielles :\n",
    "        categorical_col_name = list(preprocessor.named_transformers_['onehotencoder'].get_feature_names_out())\n",
    "    else:\n",
    "        df = preprocessor.fit_transform(df)\n",
    "        categorical_col_name = []\n",
    "    \n",
    "    # Création de la liste des noms de colonnes :\n",
    "    col_name = numerical_col_name + categorical_col_name\n",
    "    \n",
    "    # Finalisation :\n",
    "    df = pd.DataFrame(df, columns=col_name, index=index)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3db8e",
   "metadata": {},
   "source": [
    "## my_data_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f496365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def my_data_scaler(df, columns=None, method=\"standard\", scaler_all=False):\n",
    "    \"\"\"\n",
    "    Scale les données d'un DataFrame en utilisant différentes méthodes de scaling.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données à scaler.\n",
    "        columns (list): Liste des noms de colonnes à scaler. Si aucune liste n'est spécifiée, toutes les colonnes sont scalées (par défaut: None).\n",
    "        method (str): Méthode de scaling à utiliser. Les valeurs possibles sont \"standard\" (par défaut), \"minmax\", \"robust\" et \"log\".\n",
    "        scaler_all (bool): Si True, scaler toutes les colonnes du DataFrame (par défaut: False).\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame contenant les données scalées.\n",
    "    \"\"\"\n",
    "    # On vérifie si on doit appliquer le scaler sur toutes nos données\n",
    "    if columns is None and not scaler_all:\n",
    "        raise ValueError(\"Spécifiez les colonnes à scaler ou activez l'option 'scaler_all' pour scaler toutes les colonnes.\")\n",
    "    \n",
    "    if scaler_all:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    # On sélectionne la méthode de Scalage et on l'applique à nos données.\n",
    "    if method == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    elif method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    elif method == \"log\":\n",
    "        def log_scaler(data):\n",
    "            return np.log1p(data)\n",
    "        scaler = log_scaler\n",
    "    else:\n",
    "        raise ValueError(\"Méthode de scaling non valide. Les valeurs possibles sont 'standard', 'minmax', 'robust' et 'log'.\")\n",
    "\n",
    "    # On scale nos données avec la méthode sélectionnée    \n",
    "    if method == \"log\":\n",
    "        df_scaled = df[columns].apply(scaler)\n",
    "    else:\n",
    "        scaler.fit(df[columns])\n",
    "        scaled_data = scaler.transform(df[columns])\n",
    "        df_scaled = pd.DataFrame(scaled_data, columns=columns, index=df.index)\n",
    "    \n",
    "    # On remplace nos données initiales par les données scalées.\n",
    "    df = pd.concat([df.drop(columns, axis=1), df_scaled], axis=1)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878642f5",
   "metadata": {},
   "source": [
    "## My_inverse_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50307769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_inverse_log(df, columns=None, inverse_log_all=False):\n",
    "    \"\"\"\n",
    "    Effectue l'inverse du log sur les données d'un DataFrame.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données.\n",
    "        columns (list): Liste des noms de colonnes sur lesquelles appliquer l'inverse du log. Si aucune liste n'est spécifiée, toutes les colonnes sont traitées (par défaut: None).\n",
    "        inverse_log_all (bool): Si True, appliquer l'inverse du log sur toutes les colonnes du DataFrame (par défaut: False).\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame contenant les données avec l'inverse du log appliqué.\n",
    "    \"\"\"\n",
    "    # Vérifier si on doit appliquer l'inverse du log sur toutes les colonnes\n",
    "    if columns is None and not inverse_log_all:\n",
    "        raise ValueError(\"Spécifiez les colonnes sur lesquelles appliquer l'inverse du log ou activez l'option 'inverse_log_all' pour toutes les colonnes.\")\n",
    "    \n",
    "    if inverse_log_all:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    # Appliquer l'inverse du log sur les colonnes sélectionnées\n",
    "    df_inverse_log = df[columns].apply(lambda x: np.expm1(x))\n",
    "    \n",
    "    # Remplacer les données initiales par les données avec l'inverse du log appliqué\n",
    "    df_result = pd.concat([df.drop(columns, axis=1), df_inverse_log], axis=1)\n",
    "    \n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e021ea",
   "metadata": {},
   "source": [
    "# Analyse Exploratoire de Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e9975",
   "metadata": {},
   "source": [
    "## Analyse Univériée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f5c72",
   "metadata": {},
   "source": [
    "### my_analyse_univarie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac0dd414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_analyse_univarie(df, variable):\n",
    "    \"\"\"\n",
    "    Effectue une analyse univariée sur une variable donnée dans un DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - variable: str, nom de la variable à analyser\n",
    "\n",
    "    Returns:\n",
    "    - None (affiche les résultats)\n",
    "    \"\"\"\n",
    "    if variable not in df.columns:\n",
    "        print(f\"La variable '{variable}' n'est pas présente dans le DataFrame.\")\n",
    "        return\n",
    "\n",
    "    column = df[variable]\n",
    "\n",
    "    print(f\"Analyse univariée de la variable '{variable}':\\n\")\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(column):\n",
    "        # Pour les variables numériques\n",
    "        print(\"Statistiques descriptives:\")\n",
    "        print(column.describe())\n",
    "        \n",
    "        # Histogramme\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(column, kde=True)\n",
    "        plt.title(f'Distribution de la variable {variable}')\n",
    "        plt.show()\n",
    "\n",
    "        # Diagramme en boîte\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(x=column)\n",
    "        plt.title(f'Diagramme en boîte de la variable {variable}')\n",
    "        plt.show()\n",
    "\n",
    "    elif pd.api.types.is_categorical_dtype(column):\n",
    "        # Pour les variables catégorielles\n",
    "        print(\"Table de fréquence:\")\n",
    "        print(column.value_counts())\n",
    "\n",
    "        # Diagramme en secteurs\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        column.value_counts().plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "        plt.title(f'Diagramme en secteurs de la variable {variable}')\n",
    "        plt.ylabel('')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"La variable '{variable}' n'est ni numérique ni catégorielle. L'analyse univariée n'est pas prise en charge pour cette variable.\")\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "# univariate_analysis(df, \"votre_variable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dfd220",
   "metadata": {},
   "source": [
    "### my_boxplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ee32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_boxplots(df):\n",
    "    \"\"\"\n",
    "    Crée un graphique boxplot pour chaque variable d'un DataFrame.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame contenant les données.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    df.boxplot(ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988d1d9",
   "metadata": {},
   "source": [
    "### my_outliers_zscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fd87b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_outliers_zscore(df, seuil=3):\n",
    "    \"\"\"\n",
    "    Args : \n",
    "        df : (pandas.DataFrame)\n",
    "        seuil : Dégré de liberté dans le calcul du Standard Deviation. \n",
    "    Cette fonction prend en entrée un DataFrame et utilise la méthode Z-Score pour détecter les valeurs aberrantes.\n",
    "    Elle retourne un nouveau DataFrame contenant toutes les valeurs aberrantes.\n",
    "    \"\"\"\n",
    "    data = df\n",
    "    \n",
    "    # Calcule le Z-Score pour chaque colonne du DataFrame\n",
    "    z_scores = stats.zscore(data)\n",
    "\n",
    "    # Trouve toutes les valeurs dont la valeur absolue du Z-Score est supérieure à 3 (seuil de détection)\n",
    "    outliers = data[(abs(z_scores) > seuil).any(axis=1)]\n",
    "\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54a32e",
   "metadata": {},
   "source": [
    "### my_outliers_remover()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d93cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_outliers_remover(data, col):\n",
    "    Quant = data[col].quantile([0.25, 0.5, 0.75]).values\n",
    "\n",
    "    # Règle des 3 * IQR\n",
    "    lower_bound_iqr = Quant[0] - 3 * (Quant[2] - Quant[0])\n",
    "    upper_bound_iqr = Quant[2] + 3 * (Quant[2] - Quant[0])\n",
    "\n",
    "    n_total = data.shape[0]\n",
    "    n_iqr = data[(data[col] < lower_bound_iqr) | (data[col] > upper_bound_iqr)].shape[0]\n",
    "\n",
    "    if n_iqr > 0:\n",
    "        percentage_removed = (n_iqr / n_total) * 100\n",
    "        print(f'Traitement de la colonne {col}: Suppression de {n_iqr} outliers ({percentage_removed:.2f}% des données, 3 * IQR).')\n",
    "        return data[(data[col] >= lower_bound_iqr) & (data[col] <= upper_bound_iqr) | (data[col].isna())]\n",
    "    else:\n",
    "        print(f'Aucun traitement nécessaire pour la colonne {col}.')\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b939913",
   "metadata": {},
   "source": [
    "## my_histo_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c90b8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_histo_plot(data):\n",
    "    \"\"\"\n",
    "    Affiche la distribution de chacune des variables numériques du DataFrame.\n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame contenant les données à visualiser.\n",
    "    \"\"\"\n",
    "    # Vérifie s'il y a des colonnes numériques dans le DataFrame\n",
    "    numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "\n",
    "    if len(numeric_columns) == 0:\n",
    "        print(\"Aucune variable numérique à visualiser.\")\n",
    "        return\n",
    "\n",
    "    # Détermine le nombre de sous-plots nécessaire en fonction du nombre de colonnes numériques\n",
    "    num_variables = len(numeric_columns)\n",
    "    num_rows = (num_variables // 3) + (num_variables % 3)\n",
    "\n",
    "    # Définit la taille du graphique\n",
    "    plt.figure(figsize=(15, 5 * num_rows))\n",
    "\n",
    "    # Boucle à travers chaque variable numérique et trace la distribution\n",
    "    for i, variable in enumerate(numeric_columns):\n",
    "        plt.subplot(num_rows, 3, i + 1)\n",
    "        sns.histplot(data[variable], kde=True)\n",
    "        plt.title(f'Distribution de {variable}')\n",
    "        plt.xlabel(variable)\n",
    "        plt.ylabel('Fréquence')\n",
    "\n",
    "    # Ajuste l'espacement entre les sous-graphiques\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Affiche le graphique\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb321404",
   "metadata": {},
   "source": [
    "## Analyse Bivariée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9cc32",
   "metadata": {},
   "source": [
    "### my_corr_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2fd6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_corr_heatmap(data, annot=True):\n",
    "    \"\"\"\n",
    "    Calcule le coefficient de corrélation de Pearson entre toutes les paires de variables numériques d'un DataFrame.\n",
    "    Affiche seulement la moitié supérieure de la heatmap sans le miroir.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame contenant les données.\n",
    "        annot (bool or DataFrame, optional): Si True, annotera la heatmap avec les valeurs de corrélation.\n",
    "                                            Si c'est un DataFrame, utilisera ces valeurs pour l'annotation.\n",
    "                                            Par défaut, annot est True.\n",
    "    Returns:\n",
    "        sns.heatmap: heatmap contenant les coefficients de corrélation entre toutes les paires de variables numériques.\n",
    "    \"\"\"\n",
    "    # Filtrer les colonnes numériques\n",
    "    numeric_data = data.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Vérifier s'il y a au moins deux variables numériques pour calculer la corrélation\n",
    "    if numeric_data.shape[1] < 2:\n",
    "        print(\"Il doit y avoir au moins deux variables numériques pour calculer la corrélation.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.title(\"Heatmap de corrélation de Pearson\")\n",
    "    \n",
    "    # Utiliser annot si c'est un tableau, sinon, utiliser True par défaut\n",
    "    annot_data = numeric_data.corr() if annot is True else annot\n",
    "    \n",
    "    # Créer un masque pour la moitié supérieure\n",
    "    mask = np.triu(np.ones_like(annot_data, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(annot_data, annot=annot_data, cmap=\"coolwarm\", mask=mask)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e63f6f",
   "metadata": {},
   "source": [
    "### my_diagramme_violon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc0cbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_diagramme_violon(df, variables):\n",
    "    \"\"\"\n",
    "    Crée des diagrammes de violon pour une variable catégorielle et une variable numérique.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - variables: list, liste des noms de variables [variable_catégorielle, variable_numérique]\n",
    "\n",
    "    Returns:\n",
    "    - None (affiche le diagramme de violon)\n",
    "    \"\"\"\n",
    "    if len(variables) != 2:\n",
    "        print(\"La liste des variables doit contenir une variable catégorielle et une variable numérique.\")\n",
    "        return\n",
    "\n",
    "    variable_categorique, variable_numerique = variables\n",
    "\n",
    "    if variable_categorique not in df.columns:\n",
    "        print(f\"La variable catégorielle '{variable_categorique}' n'est pas présente dans le DataFrame.\")\n",
    "        return\n",
    "\n",
    "    if variable_numerique not in df.columns:\n",
    "        print(f\"La variable numérique '{variable_numerique}' n'est pas présente dans le DataFrame.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.violinplot(x=variable_categorique, y=variable_numerique, data=df)\n",
    "    plt.title(f'Diagramme de violon pour {variable_numerique} en fonction de {variable_categorique}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb34156",
   "metadata": {},
   "source": [
    "### my_diagramme_barplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7701675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_diagramme_barplot(df, variables):\n",
    "    \"\"\"\n",
    "    Crée des barplots pour une variable catégorielle et une variable numérique.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - variables: list, liste des noms de variables [variable_catégorielle, variable_numérique]\n",
    "\n",
    "    Returns:\n",
    "    - None (affiche le barplot)\n",
    "    \"\"\"\n",
    "    if len(variables) != 2:\n",
    "        print(\"La liste des variables doit contenir une variable catégorielle et une variable numérique.\")\n",
    "        return\n",
    "\n",
    "    variable_categorique, variable_numerique = variables\n",
    "\n",
    "    if variable_categorique not in df.columns:\n",
    "        print(f\"La variable catégorielle '{variable_categorique}' n'est pas présente dans le DataFrame.\")\n",
    "        return\n",
    "\n",
    "    if variable_numerique not in df.columns:\n",
    "        print(f\"La variable numérique '{variable_numerique}' n'est pas présente dans le DataFrame.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.barplot(x=variable_categorique, y=variable_numerique, data=df)\n",
    "    plt.title(f'Barplot pour {variable_numerique} en fonction de {variable_categorique}')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Ajout des étiquettes de données\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe08c34",
   "metadata": {},
   "source": [
    "## Analyse Multivariée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c033ffc",
   "metadata": {},
   "source": [
    "### my_anova_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "903ec7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_anova_test(df, variable_dependante, variable_independante):\n",
    "    \"\"\"\n",
    "    Effectue un test d'ANOVA sur un ensemble de groupes dans un DataFrame.\n",
    "    Inclut également des tests de normalité des résidus et d'homogénéité des variances.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - variable_dependante: str, nom de la variable dépendante (numérique)\n",
    "    - variable_independante: str, nom de la variable indépendante (catégorielle)\n",
    "\n",
    "    Returns:\n",
    "    - result: DataFrame, tableau de résultats ANOVA et tests associés\n",
    "    \"\"\"\n",
    "    if variable_dependante not in df.columns:\n",
    "        print(f\"La variable dépendante '{variable_dependante}' n'est pas présente dans le DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    if variable_independante not in df.columns:\n",
    "        print(f\"La variable indépendante '{variable_independante}' n'est pas présente dans le DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    # Séparation des données en groupes\n",
    "    groupes = [data[variable_dependante] for groupe, data in df.groupby(variable_independante)]\n",
    "\n",
    "    # Test de normalité des résidus (Shapiro-Wilk)\n",
    "    shapiro_results = shapiro(df[variable_dependante])\n",
    "    shapiro_statistic, shapiro_p_value = shapiro_results\n",
    "\n",
    "    # Test d'homogénéité des variances (Levene)\n",
    "    levene_results = levene(*groupes)\n",
    "    levene_statistic, levene_p_value = levene_results\n",
    "\n",
    "    # Vérification des hypothèses pour ANOVA\n",
    "    hypotheses_check = pd.DataFrame({\n",
    "        'Hypothèse': ['Normalité des résidus', 'Homogénéité des variances'],\n",
    "        'Statistique de test': [shapiro_statistic, levene_statistic],\n",
    "        'p-valeur': [shapiro_p_value, levene_p_value],\n",
    "        'Hypothèse nulle': ['Les résidus sont normalement distribués', 'Les variances sont égales entre les groupes']\n",
    "    })\n",
    "\n",
    "    # Vérification si les tests d'hypothèses sont satisfaits\n",
    "    tests_passed = all(p_value > 0.05 for p_value in [shapiro_p_value, levene_p_value])\n",
    "\n",
    "    if not tests_passed:\n",
    "        print(\"Les hypothèses pour l'ANOVA ne sont pas satisfaites. Veuillez vérifier la normalité des résidus et l'homogénéité des variances.\")\n",
    "        return hypotheses_check\n",
    "\n",
    "    # Test d'ANOVA\n",
    "    result_statistic, p_value = f_oneway(*groupes)\n",
    "\n",
    "    # Création du tableau de résultats\n",
    "    result = pd.DataFrame({\n",
    "        'Source de variation': [variable_independante],\n",
    "        'SS entre les groupes': [result_statistic],\n",
    "        'degrés de liberté entre les groupes': [len(groupes) - 1],\n",
    "        'SS à l\\'intérieur des groupes': [sum((data - data.mean())**2 for data in groupes)],\n",
    "        'degrés de liberté à l\\'intérieur des groupes': [len(df) - len(groupes)],\n",
    "        'SS totale': [sum((df[variable_dependante] - df[variable_dependante].mean())**2)],\n",
    "        'degrés de liberté totaux': [len(df) - 1],\n",
    "        'MS entre les groupes': [result_statistic / (len(groupes) - 1)],\n",
    "        'MS à l\\'intérieur des groupes': [sum((data - data.mean())**2 for data in groupes) / (len(df) - len(groupes))],\n",
    "        'F-statistique': [result_statistic],\n",
    "        'p-valeur': [p_value]\n",
    "    })\n",
    "\n",
    "    return pd.concat([result, hypotheses_check], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9f2ad",
   "metadata": {},
   "source": [
    "### my_kruskal_wallis_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ba55256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kruskal_wallis_test(df, variable_dependante, variable_independante):\n",
    "    \"\"\"\n",
    "    Effectue un test de Kruskal-Wallis sur un ensemble de groupes dans un DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - variable_dependante: str, nom de la variable dépendante (numérique)\n",
    "    - variable_independante: str, nom de la variable indépendante (catégorielle)\n",
    "\n",
    "    Returns:\n",
    "    - result: DataFrame, tableau de résultats Kruskal-Wallis\n",
    "    \"\"\"\n",
    "    if variable_dependante not in df.columns:\n",
    "        print(f\"La variable dépendante '{variable_dependante}' n'est pas présente dans le DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    if variable_independante not in df.columns:\n",
    "        print(f\"La variable indépendante '{variable_independante}' n'est pas présente dans le DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    # Séparation des données en groupes\n",
    "    groupes = [data[variable_dependante].dropna() for _, data in df.groupby(variable_independante)]\n",
    "\n",
    "    # Test de Kruskal-Wallis\n",
    "    kruskal_statistic, kruskal_p_value = kruskal(*groupes)\n",
    "\n",
    "    # Création du tableau de résultats\n",
    "    result = pd.DataFrame({\n",
    "        'Source de variation': [variable_independante],\n",
    "        'Statistique de test (Kruskal-Wallis)': [kruskal_statistic],\n",
    "        'p-valeur (Kruskal-Wallis)': [kruskal_p_value]\n",
    "    })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a24afc",
   "metadata": {},
   "source": [
    "## Tests statistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48540bb4",
   "metadata": {},
   "source": [
    "### my_pearson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfa737bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def my_pearson(x, y):\n",
    "    \"\"\"\n",
    "    Calcule le coefficient de corrélation de Pearson entre deux variables x et y.\n",
    "    \n",
    "    Args:\n",
    "    - x, y (array-like): Deux tableaux de données unidimensionnels.\n",
    "    \n",
    "    Returns:\n",
    "    - pearson_coef (float): Le coefficient de corrélation de Pearson.\n",
    "    - p_value (float): La p-value associée au test d'hypothèse.\n",
    "    \n",
    "    \"\"\"\n",
    "    pearson_coef, p_value = pearsonr(x, y)\n",
    "    if p_value < 0.05:\n",
    "        print(\"P-value :\", p_value, \"L'hypothèse H1 est vérifiée, il existe une corrélation significative entre ces deux variables.\")\n",
    "    else:\n",
    "        print(\"P-value :\", p_value, \"L'hypothèse H0 est vérifiée, il ne semble pas exister de corrélation significative entre ces deux variables.\")\n",
    "    return pearson_coef, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026616e",
   "metadata": {},
   "source": [
    "## Projection des individus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08d4b2",
   "metadata": {},
   "source": [
    "### my_projection_individus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b482591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_projection_individus(   X_projected, \n",
    "                                x_y, \n",
    "                                pca=None, \n",
    "                                labels = None,\n",
    "                                clusters=None, \n",
    "                                alpha=1,\n",
    "                                figsize=[10,8], \n",
    "                                marker=\".\" ):\n",
    "    \"\"\"\n",
    "    Affiche la projection des individus\n",
    "\n",
    "    Positional arguments : \n",
    "    -------------------------------------\n",
    "    X_projected : np.array, pd.DataFrame, list of list : la matrice des points projetés\n",
    "    x_y : list ou tuple : le couple x,y des plans à afficher, exemple [0,1] pour F1, F2\n",
    "\n",
    "    Optional arguments : \n",
    "    -------------------------------------\n",
    "    pca : sklearn.decomposition.PCA : un objet PCA qui a été fit, cela nous permettra d'afficher la variance de chaque composante, default = None\n",
    "    labels : list ou tuple : les labels des individus à projeter, default = None\n",
    "    clusters : list ou tuple : la liste des clusters auquel appartient chaque individu, default = None\n",
    "    alpha : float in [0,1] : paramètre de transparence, 0=100% transparent, 1=0% transparent, default = 1\n",
    "    figsize : list ou tuple : couple width, height qui définit la taille de la figure en inches, default = [10,8] \n",
    "    marker : str : le type de marker utilisé pour représenter les individus, points croix etc etc, default = \".\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Transforme X_projected en np.array\n",
    "    X_ = np.array(X_projected)\n",
    "\n",
    "    # On définit la forme de la figure si elle n'a pas été donnée\n",
    "    if not figsize: \n",
    "        figsize = (7,6)\n",
    "\n",
    "    # On gère les labels\n",
    "    if  labels is None : \n",
    "        labels = []\n",
    "    try : \n",
    "        len(labels)\n",
    "    except Exception as e : \n",
    "        raise e\n",
    "\n",
    "    # On vérifie la variable axis \n",
    "    if not len(x_y) ==2 : \n",
    "        raise AttributeError(\"2 axes sont demandées\")   \n",
    "    if max(x_y )>= X_.shape[1] : \n",
    "        raise AttributeError(\"la variable axis n'est pas bonne\")   \n",
    "\n",
    "    # on définit x et y \n",
    "    x, y = x_y\n",
    "\n",
    "    # Initialisation de la figure       \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # On vérifie s'il y a des clusters ou non\n",
    "    c = None if clusters is None else clusters\n",
    " \n",
    "    # Les points    \n",
    "    # plt.scatter(   X_[:, x], X_[:, y], alpha=alpha, \n",
    "    #                     c=c, cmap=\"Set1\", marker=marker)\n",
    "    sns.scatterplot(data=None, x=X_[:, x], y=X_[:, y], hue=c)\n",
    "\n",
    "    # Si la variable pca a été fournie, on peut calculer le % de variance de chaque axe \n",
    "    if pca : \n",
    "        v1 = str(round(100*pca.explained_variance_ratio_[x]))  + \" %\"\n",
    "        v2 = str(round(100*pca.explained_variance_ratio_[y]))  + \" %\"\n",
    "    else : \n",
    "        v1=v2= ''\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    ax.set_xlabel(f'F{x+1} {v1}')\n",
    "    ax.set_ylabel(f'F{y+1} {v2}')\n",
    "\n",
    "    # Valeur x max et y max\n",
    "    x_max = np.abs(X_[:, x]).max() *1.1\n",
    "    y_max = np.abs(X_[:, y]).max() *1.1\n",
    "\n",
    "    # On borne x et y \n",
    "    ax.set_xlim(left=-x_max, right=x_max)\n",
    "    ax.set_ylim(bottom= -y_max, top=y_max)\n",
    "\n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-x_max, x_max], [0, 0], color='grey', alpha=0.8)\n",
    "    plt.plot([0,0], [-y_max, y_max], color='grey', alpha=0.8)\n",
    "\n",
    "    # Affichage des labels des points\n",
    "    if len(labels) : \n",
    "        for i,(_x,_y) in enumerate(X_[:,[x,y]]):\n",
    "            plt.text(_x, _y+0.05, labels[i], fontsize='14', ha='center',va='center') \n",
    "\n",
    "    # Titre et display\n",
    "    plt.title(f\"Projection des individus (sur F{x+1} et F{y+1})\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163cebc",
   "metadata": {},
   "source": [
    "## Analyse en Composante Principales (ACP ou PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8b43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1de05c3e",
   "metadata": {},
   "source": [
    "### my_pca_coude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7843d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca_coude(data, n_components_max=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Execute la méthode du coude pour déterminer le nombre idéal de composantes principales d'une ACP.\n",
    "\n",
    "    Paramètre:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Les données à analyser\n",
    "    n_components_max : int, optionnel\n",
    "        Le nombre maximum de composantes principales à tester. Aucune limite par défaut (i.e. n_components_max=len(data.columns)-1).\n",
    "    \n",
    "    Sortie:\n",
    "    --------\n",
    "        Le graphique coude.\n",
    "    \"\"\"\n",
    "    # on définit le nombre de composantes principales à tester. \n",
    "    if n_components_max is None:\n",
    "        # Si None, on selectionne le nombre de colonne du dataframe -1\n",
    "        n_components_max = len(data.columns) - 1\n",
    "\n",
    "    # On initialise notre PCA    \n",
    "    pca = PCA(n_components=n_components_max)\n",
    "    # On entraine notre PCA\n",
    "    pca.fit(data)\n",
    "    # On récupère la variance de chaque composantes\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    # On additionne chaque variance pour avoir la somme cumulée\n",
    "    cumsum_var = np.cumsum(explained_var)\n",
    "\n",
    "    # On gènère notre graphique\n",
    "    plt.plot(range(1, n_components_max+1), cumsum_var, 'bo-')\n",
    "    plt.xlabel('Nombre de composantes principales')\n",
    "    plt.ylabel('Somme de la variance')\n",
    "    plt.title('Méthode du coude pour ACP')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0bd82",
   "metadata": {},
   "source": [
    "### my_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebbb24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca(n_components, data, x_y, scaling=False, labels=None):\n",
    "    \"\"\"\n",
    "    Effectue une Analyse en Composantes Principales (PCA) sur les données.\n",
    "    \n",
    "    Args:\n",
    "    - n_components (int): Le nombre de composantes principales à extraire.\n",
    "    - data (pandas DataFrame): Le dataframe contenant les données à analyser.\n",
    "    - x_y (tuple): Un tuple contenant les index de deux composantes à comparer.\n",
    "    - scaling (bool): Si True, standardise les données avant la PCA.\n",
    "    - labels (array-like): Les labels des clusters correspondants aux données.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \n",
    "    Example:\n",
    "    my_pca(3, df, (0, 1), True, labels=cluster_labels)\n",
    "    \"\"\"\n",
    "    # Définition des données\n",
    "    X = data.values\n",
    "    # Définition des noms des index (lignes) de notre jeu de données\n",
    "    names = data.index\n",
    "    # Définition des noms des colonnes de notre jeu de données\n",
    "    features = data.columns\n",
    "    \n",
    "    if scaling is True:\n",
    "        # Scaling\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        data = pd.DataFrame(scaler.fit_transform(X), columns=features)\n",
    "        \n",
    "    # PCA\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    # On entraîne nos données\n",
    "    pca.fit(data)\n",
    "    \n",
    "    # Calcul des coordonnées des individus sur les axes d'inertie\n",
    "    projected = pca.transform(data)\n",
    "    \n",
    "    # On calcule le ratio de la variance\n",
    "    pca.explained_variance_ratio_\n",
    "    scree = (pca.explained_variance_ratio_ * 100).round(2)\n",
    "    # En cumulé\n",
    "    scree_cum = scree.cumsum().round()\n",
    "    print(\"Explained Variance Ratio:\", scree_cum)\n",
    "    \n",
    "    # Affichage graphique de l'Eboulis des valeurs propres\n",
    "    x_list = range(1, n_components+1)\n",
    "    list(x_list)\n",
    "    \n",
    "    plt.bar(x_list, scree)\n",
    "    plt.plot(x_list, scree_cum, c=\"red\", marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Définition des composantes principales\n",
    "    pcs = pca.components_\n",
    "    pcs = pd.DataFrame(pcs)\n",
    "    pcs.columns = features\n",
    "    pcs.index = [f\"F{i}\" for i in x_list]\n",
    "    pcs.round(2)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    sns.heatmap(pcs.T, vmin=-1, vmax=1, annot=True, cmap=\"coolwarm\", fmt=\"0.2f\")\n",
    "    plt.title(\"Heatmap des corrélation\")\n",
    "    plt.xlabel(\"Composante\")\n",
    "    \n",
    "    # Correlation graph\n",
    "    \n",
    "    # Extrait x et y\n",
    "    x, y = x_y\n",
    "\n",
    "    # Taille de l'image (en inches)\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "    # Pour chaque composante :\n",
    "    for i in range(0, pca.components_.shape[1]):\n",
    "\n",
    "        # Les flèches\n",
    "        ax.arrow(0, 0,\n",
    "                 pca.components_[x, i],\n",
    "                 pca.components_[y, i],\n",
    "                 head_width=0.07,\n",
    "                 head_length=0.07,\n",
    "                 width=0.02, )\n",
    "\n",
    "        # Les labels\n",
    "        plt.text(pca.components_[x, i] + 0.05,\n",
    "                 pca.components_[y, i] + 0.05,\n",
    "                 features[i])\n",
    "\n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "    plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    plt.xlabel('F{} ({}%)'.format(x+1, round(100*pca.explained_variance_ratio_[x], 1)))\n",
    "    plt.ylabel('F{} ({}%)'.format(y+1, round(100*pca.explained_variance_ratio_[y], 1)))\n",
    "\n",
    "    # Titre du graphique\n",
    "    plt.title(\"Cercle des corrélations (F{} et F{})\".format(x+1, y+1))\n",
    "\n",
    "    # Le cercle\n",
    "    an = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n",
    "\n",
    "    # Axes et display\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    if labels is not None:\n",
    "        # Calcul des centroides des clusters\n",
    "        n_clusters = len(np.unique(labels))\n",
    "        centroids = np.zeros((n_clusters, n_components))\n",
    "        for i in range(n_clusters):\n",
    "            centroids[i] = np.mean(projected[labels == i], axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 12))\n",
    "        palette = sns.color_palette(\"hls\", n_clusters)\n",
    "\n",
    "        # Tracé du graphique de projection des individus\n",
    "        unique_labels = np.unique(labels)\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            plt.scatter(projected[labels == label, 0], projected[labels == label, 1], label=label, c=[palette[i]])\n",
    "\n",
    "        plt.scatter(centroids[:, 0], centroids[:, 1], marker='s', color=palette, s=200, label='Centroids', edgecolors=\"black\")\n",
    "        plt.xlabel('F{} ({}%)'.format(x+1, round(100*pca.explained_variance_ratio_[x], 1)))\n",
    "        plt.ylabel('F{} ({}%)'.format(y+1, round(100*pca.explained_variance_ratio_[y], 1)))\n",
    "        plt.title('Projection des individus sur les axes d\\'inertie')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2b140",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e293bf",
   "metadata": {},
   "source": [
    "### my_kmeans_coude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba86471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kmeans_coude(data, cluster_max=10, graphique=True, numerical_encoder=None, categorical_encoder=None):\n",
    "    \"\"\"\n",
    "    Utilise la méthode du coude pour déterminer le nombre optimal de clusters à utiliser dans l'algorithme K-Means.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : DataFrame\n",
    "        Le jeu de données à analyser.\n",
    "    cluster_max : int, optional\n",
    "        Le nombre maximal de clusters à tester.\n",
    "    graphique : bool, optional\n",
    "        Indique si le graphique doit être affiché.\n",
    "    numerical_encoder : Transformer, optional\n",
    "        Encodeur à utiliser pour les colonnes numériques (e.g., StandardScaler(), MinMaxScaler(), RobustScaler()).\n",
    "    categorical_encoder : Transformer, optional\n",
    "        Encodeur à utiliser pour les colonnes catégorielles (e.g., OneHotEncoder()).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Le nombre de clusters optimal déterminé par la méthode du coude.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Séparation des colonnes numériques et catégorielles\n",
    "    numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Création des transformers\n",
    "    transformers = []\n",
    "    if numerical_encoder and numeric_features:\n",
    "        transformers.append(('num', numerical_encoder, numeric_features))\n",
    "    if categorical_encoder and categorical_features:\n",
    "        transformers.append(('cat', categorical_encoder, categorical_features))\n",
    "    \n",
    "    # Création du ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(transformers)\n",
    "    \n",
    "    # Création du pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    \n",
    "    # Transformation des données\n",
    "    data_transformed = pipeline.fit_transform(data)\n",
    "    \n",
    "    # Détermination de l'inertie pour différents nombres de clusters\n",
    "    inertia = []\n",
    "    k_list = range(1, cluster_max)\n",
    "    for k in k_list:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data_transformed)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Graphique\n",
    "    if graphique:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        ax.set_ylabel(\"Inertia\")\n",
    "        ax.set_xlabel(\"Nombre de clusters\")\n",
    "        ax.plot(k_list, inertia)\n",
    "        plt.show()\n",
    "    \n",
    "    # Trouver l'indice du nombre de clusters optimal\n",
    "    optimal_index = np.argmin(np.diff(inertia)) + 1\n",
    "\n",
    "    # Récupérer le nombre de clusters optimal\n",
    "    n_clusters = optimal_index + 1\n",
    "    #print(\"D'après la méthode du coude, le nombre de clusters optimal est :\", n_clusters)\n",
    "    #return n_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80418d0",
   "metadata": {},
   "source": [
    "### my_silhouette_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31857309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_silhouette_method(data, max_clusters, numerical_encoder=None, categorical_encoder=None):\n",
    "    \"\"\"\n",
    "    Cette fonction effectue une analyse de silhouette pour déterminer le nombre optimal de clusters à utiliser pour\n",
    "    la méthode de clustering de KMeans.\n",
    "\n",
    "    :param data: DataFrame avec les données à utiliser pour l'analyse\n",
    "    :param max_clusters: Nombre maximum de clusters à tester\n",
    "    :param numerical_encoder: Encodeur pour les colonnes numériques (e.g., StandardScaler(), MinMaxScaler(), RobustScaler())\n",
    "    :param categorical_encoder: Encodeur pour les colonnes catégorielles (e.g., OneHotEncoder())\n",
    "    :return: Le modèle KMeans avec le nombre optimal de clusters selon l'analyse de silhouette\n",
    "    \"\"\"\n",
    "    \n",
    "    # Séparation des colonnes numériques et catégorielles\n",
    "    numeric_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Création des transformers\n",
    "    transformers = []\n",
    "    if numerical_encoder and numeric_features:\n",
    "        transformers.append(('num', numerical_encoder, numeric_features))\n",
    "    if categorical_encoder and categorical_features:\n",
    "        transformers.append(('cat', categorical_encoder, categorical_features))\n",
    "    \n",
    "    # Création du ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(transformers)\n",
    "    \n",
    "    # Création du pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    \n",
    "    # Transformation des données\n",
    "    data_transformed = pipeline.fit_transform(data)\n",
    "    \n",
    "    # Liste pour stocker les scores de silhouette\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Boucle pour tester différents nombres de clusters\n",
    "    for num_clusters in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=10, random_state=42)\n",
    "        kmeans.fit(data_transformed)\n",
    "        cluster_labels = kmeans.labels_\n",
    "\n",
    "        # Calculer le score de silhouette pour chaque point\n",
    "        silhouette_avg = silhouette_score(data_transformed, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Trouver le nombre optimal de clusters en se basant sur le score de silhouette le plus élevé\n",
    "    optimal_num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "\n",
    "    # Afficher le nombre optimal de cluster\n",
    "    #print(\"Le nombre optimal de cluster est :\", optimal_num_clusters)\n",
    "\n",
    "    # Entraîner le modèle KMeans avec le nombre optimal de clusters et retourner le modèle\n",
    "    kmeans = KMeans(n_clusters=optimal_num_clusters, init='k-means++', max_iter=100, n_init=10, random_state=42)\n",
    "    kmeans.fit(data_transformed)\n",
    "    \n",
    "    plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "    plt.xlabel('Nombre de clusters')\n",
    "    plt.ylabel('Score Silhouette')\n",
    "    plt.show()\n",
    "    \n",
    "    return kmeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1ac79",
   "metadata": {},
   "source": [
    "### my_kmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c99e58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kmeans(data, n_clusters, n_runs=10, graphique=True):\n",
    "    \n",
    "    \n",
    "    best_kmeans = None\n",
    "    best_labels = None\n",
    "    best_inertia = np.inf\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        # On initialise le K-Means sur nos données\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        # On entraine nos données\n",
    "        kmeans.fit(data)\n",
    "        # On récupère le numéro de chaque cluster dans une variable\n",
    "        labels = kmeans.labels_\n",
    "        \n",
    "        # Calcul de l'inertie (somme des distances au carré)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        if inertia < best_inertia:\n",
    "            best_inertia = inertia\n",
    "            best_kmeans = kmeans\n",
    "            best_labels = labels\n",
    "    \n",
    "    # Meilleur modèle K-Means\n",
    "    kmeans = best_kmeans\n",
    "    labels = best_labels\n",
    "    \n",
    "       \n",
    "    \n",
    "    # PCA\n",
    "    # On initialise le PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    # On entraine nos données\n",
    "    pca.fit(data)\n",
    "    # On récupère le score de chaque composantes\n",
    "    pca_scores = pca.transform(data)\n",
    "    # on stock ces information dans un DataFrame\n",
    "    pca_df = pd.DataFrame(pca_scores, columns=['PC1', 'PC2'])\n",
    "    if graphique==True:\n",
    "        # Graphique\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=labels, palette=sns.color_palette(\"hls\", n_clusters), data=pca_df, ax=ax, alpha=0.5)\n",
    "\n",
    "        #plt.scatter(pca_scores[:, 0], pca_scores[:, 1], alpha=0.5, edgecolors='black', linewidths=1.4, s=85)\n",
    "\n",
    "\n",
    "        # On stock nos centroid dans une variable pour pouvoir la visualier plus tard.\n",
    "        centroids = kmeans.cluster_centers_\n",
    "\n",
    "        # On affiche un Dataframe avec chaque centroid pour chaque variable\n",
    "        display(pd.DataFrame(centroids, columns=data.columns))\n",
    "\n",
    "        centroids = np.zeros((n_clusters, 2))\n",
    "        for i in range(n_clusters):\n",
    "            centroids[i] = np.mean(pca_scores[labels == i], axis=0)\n",
    "\n",
    "\n",
    "        # On affiche les centroid sur le graphique\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], c=sns.color_palette(\"hls\", n_clusters), marker='s', s=100,edgecolors='black')\n",
    "        ax.set_title(\"Projection des individus\")\n",
    "        ax.set_xlabel(\"PC1 ({:.2f}%)\".format(pca.explained_variance_ratio_[0] * 100))\n",
    "        ax.set_ylabel(\"PC2 ({:.2f}%)\".format(pca.explained_variance_ratio_[1] * 100))\n",
    "        plt.show()\n",
    "    # On ajouter une nouvelle colonnes dans notre DataFrame initiale avec le numéro de chaque cluster.\n",
    "    data[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebb31373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_individuals(X, labels):\n",
    "    # Conversion des données X en tableau NumPy si elles sont de type DataFrame\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "\n",
    "    # Création de la figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Récupération des clusters uniques\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Définition des couleurs des clusters\n",
    "    colormap = plt.cm.get_cmap('viridis', len(unique_labels))\n",
    "\n",
    "    # Tracé des individus par cluster\n",
    "    for label in unique_labels:\n",
    "        cluster_points = X[labels == label]\n",
    "        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=colormap(label),\n",
    "                    alpha=0.5, edgecolors='black', linewidths=1.4, s=85)\n",
    "\n",
    "    # Étiquetage des axes et de la légende\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend(unique_labels, title='Cluster')\n",
    "\n",
    "    # Affichage du graphique\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9108a4",
   "metadata": {},
   "source": [
    "### my_all_kmeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a6226bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_all_kmeans(data, cluster_max=10, graphique=False):\n",
    "    \"\"\"\n",
    "    Cette fonction applique la méthode du coude pour trouver le nombre de clusters optimal, puis effectue l'algorithme de K-means sur les données passées en paramètre avec le nombre de clusters optimal trouvé. Elle prend en entrée les données à traiter, le nombre maximum de clusters à considérer, ainsi qu'un paramètre optionnel graphique qui, s'il est True, permet de visualiser les clusters trouvés à travers un pairplot.\n",
    "    \n",
    "    Args:\n",
    "        data : Les données à traiter.\n",
    "        cluster_max : Le nombre maximum de clusters à considérer.\n",
    "        graphique : Si True, permet de visualiser les clusters trouvés à travers un pairplot.\n",
    "    \"\"\"\n",
    "    #Méthode du coude\n",
    "    inertia = []\n",
    "    k_list = range(1, cluster_max)\n",
    "    for k in k_list :\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    #Graphique\n",
    "    fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "    ax.set_ylabel(\"Inertie\")\n",
    "    ax.set_xlabel(\"Nombre de clusters\")\n",
    "    ax.plot(k_list, inertia)\n",
    "    plt.show()\n",
    "    \n",
    "    # Trouver l'indice du nombre de clusters optimal\n",
    "    optimal_index = np.argmin(np.diff(inertia)) + 1\n",
    "    \n",
    "    # Récupérer le nombre de clusters optimal\n",
    "    n_clusters = optimal_index + 1\n",
    "    print(\"D'après la méthode du coude, le nombre de cluster optimal est :\",n_clusters)\n",
    "    \n",
    "\n",
    "    if graphique is True:\n",
    "        sns.pairplot(data, hue=\"cluster\")\n",
    "        plt.figure(figsize=(25,25))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa1d1a",
   "metadata": {},
   "source": [
    "## Dendrogramme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d3aadd",
   "metadata": {},
   "source": [
    "### my_dendrogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f24b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dendrogram(data, nombre_cluster, methode=\"ward\"):\n",
    "    \"\"\"\n",
    "    Cette fonction effectue l'algorithme de clustering hiérarchique sur les données passées en paramètre et affiche le dendrogramme correspondant. Elle prend en entrée les données à traiter, le nombre de clusters désiré, ainsi qu'un paramètre optionnel methode qui permet de spécifier la méthode à utiliser pour le clustering.\n",
    "    \n",
    "    Args:\n",
    "        data : Les données à traiter.\n",
    "        nombre_cluster : Le nombre de clusters désiré.\n",
    "        methode : La méthode à utiliser pour le clustering.\n",
    "    \"\"\"\n",
    "    Z = linkage(data, method=methode)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(90, 40))\n",
    "\n",
    "    _ = dendrogram(Z, p=nombre_cluster, truncate_mode=\"lastp\", ax=ax,labels=data.index)\n",
    "\n",
    "    # Récupération des clusters\n",
    "    clusters = fcluster(Z, t=nombre_cluster, criterion='maxclust')\n",
    "    \n",
    "    #Index trié des groupes\n",
    "    idg = np.argsort(clusters)\n",
    "    \n",
    "    #Affichage des individus selon leurs groupes\n",
    "    df_groupes_cah = pd.DataFrame(data.index[idg], clusters[idg]).reset_index()\n",
    "    df_groupes_cah = df_groupes_cah.rename(columns={'index':'Groupe'})\n",
    "    \n",
    "    #Jointure interne nécessaire pour parvenir à agréger nos données\n",
    "    df_new = pd.merge(data, df_groupes_cah, on=data.index)\n",
    "    \n",
    "    # Affichage des clusters\n",
    "    for i in range(1, nombre_cluster + 1):\n",
    "        print(f\"Cluster {i}: {data.index[clusters == i].tolist()}\")\n",
    "\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.ylabel(\"Distance.\")\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.tick_params(axis='x', labelsize=30)\n",
    "    plt.show()\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475e6f1",
   "metadata": {},
   "source": [
    "## PCA + Projection des individus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90bbf6",
   "metadata": {},
   "source": [
    "### my_pca_proj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76137cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_pca_proj(data,\n",
    "                n_components,  \n",
    "                x_y,\n",
    "                center=None,\n",
    "                projection=False,\n",
    "                sup_columns=None,\n",
    "                sup_row=None,\n",
    "                scaling=False,\n",
    "                type_scaling=\"standard\",\n",
    "                labels = None, \n",
    "                clusters=None,\n",
    "                transform_data=None,\n",
    "                alpha=1,\n",
    "                figsize=[10,8], \n",
    "                marker=\".\" ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cette fonction effectue une analyse en composantes principales (PCA) sur les données passées en paramètre et projette les données dans l'espace réduit défini par les premières composantes principales. Elle prend en entrée les données à traiter ainsi que plusieurs paramètres optionnels pour spécifier des options de pré-traitement des données (suppression de colonnes ou de lignes, scaling), le nombre de composantes principales souhaité, le type de scaling à appliquer, des labels ou des clusters si on souhaite colorer les points du graphique obtenu, et des options d'affichage.\n",
    "    \n",
    "    Args:\n",
    "        data : Les données à traiter.\n",
    "        n_components : Le nombre de composantes principales souhaité.\n",
    "        x_y : Les deux composantes principales à afficher sur le graphique.\n",
    "        sup_columns : Les noms des colonnes à supprimer (optionnel).\n",
    "        sup_row : Les noms des lignes à supprimer (optionnel).\n",
    "        scaling : Si True, effectue un scaling sur les données (optionnel).\n",
    "        type_scaling : Le type de scaling à appliquer (standard ou robust) (optionnel).\n",
    "        labels : Les labels à afficher sur le graphique (optionnel).\n",
    "        clusters : Les clusters à afficher sur le graphique (optionnel).\n",
    "        transform_data: option pour la transformation des données, possible choix : 'log' ou 'sqrt', de type str ou None (optionnel).\n",
    "        alpha: niveau de transparence des points sur le graphique, de type float(optionnel).\n",
    "        figsize: taille de la figure à afficher, de type list(optionnel).\n",
    "        marker: forme des points à afficher sur le graphique, de type str (optionnel).\n",
    "\n",
    "        return: None (la fonction affiche les résultats graphiques directement\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "                                                    # My_pca\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Si le paramètre sup_columns n'est pas null, alors il faut supprimer les colonnes en paramètre\n",
    "    if sup_columns is not None:\n",
    "        data = data.drop(sup_columns, axis=1)\n",
    "    \n",
    "    # Si le paramètre sup_row n'est pas null, alors il faut supprimer les colonnes en paramètre\n",
    "    if sup_row is not None:\n",
    "        data = data.drop(sup_row, axis=0)\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Vérifie le type de transformation de données souhaité\n",
    "    # Log() -> Voir log négatif : valeur absolu -> log -> repasser en négatif\n",
    "    if transform_data == \"log\":\n",
    "        for i in data.all():\n",
    "            if (i == 0) or (i < 0):\n",
    "                return print(\"Log() impossible : Valeur égale ou inférieur à 0 !\")\n",
    "            else:\n",
    "                data = np.log(data)\n",
    "    # Racine carré            \n",
    "    elif transform_data == \"sqrt\":\n",
    "        for i in data.columns:\n",
    "            data[i] = np.sqrt(data[[i]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Définition des données\n",
    "    X = data.values\n",
    "    # Définition des noms des index (lignes) de notre jeu de données\n",
    "    names = data.index\n",
    "    # Définition des noms des colonnes de notre jeu de données\n",
    "    features = data.columns\n",
    "    \n",
    "    # Vérifie le type de scaling souhaité\n",
    "    #Standard Scaling\n",
    "    if scaling is True:\n",
    "        if type_scaling ==\"standard\":\n",
    "            # Standard Scaling\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "    #Robust Scaling\n",
    "        elif type_scaling==\"robust\":\n",
    "            # Robust Scaling\n",
    "            scaler = preprocessing.RobustScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "    # Données non scalé\n",
    "    else:\n",
    "        X_scaled = X\n",
    "    \n",
    "    # PCA\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    # On entraine nos données\n",
    "    pca.fit(X_scaled)\n",
    "    # On calcule le ratio de la variance\n",
    "    pca.explained_variance_ratio_\n",
    "    scree = (pca.explained_variance_ratio_*100).round(2)\n",
    "    # En cumulé\n",
    "    scree_cum = scree.cumsum().round()\n",
    "    print(\"Explained Variance Ratio :\",scree_cum)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Affichage graphique de l'Eboulis des valeurs propres\n",
    "    x_list = range(1, n_components+1)\n",
    "    list(x_list)\n",
    "    \n",
    "    plt.bar(x_list, scree)\n",
    "    plt.plot(x_list, scree_cum,c=\"red\",marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Définition des composantes principales\n",
    "    pcs = pca.components_\n",
    "    pcs = pd.DataFrame(pcs)\n",
    "    pcs.columns = features\n",
    "    pcs.index = [f\"F{i}\" for i in x_list]\n",
    "    pcs.round(2)\n",
    "  \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    sns.heatmap(pcs.T, vmin=-1, vmax=1, annot=True, cmap=\"coolwarm\", fmt=\"0.2f\")\n",
    "    plt.title(\"Tableau de corrélation ACP\")\n",
    "    plt.xlabel(\"Composante\")\n",
    "     #------------------------------------------------------------------------------------------------\n",
    "    \n",
    "                                                # Correlation graph\n",
    "        \n",
    "     #------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Extrait x et y \n",
    "    x,y=x_y\n",
    "\n",
    "    # Taille de l'image (en inches)\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "    # Pour chaque composante : \n",
    "    for i in range(0, pca.components_.shape[1]):\n",
    "\n",
    "        # Les flèches\n",
    "        ax.arrow(0,0, \n",
    "                pca.components_[x, i],  \n",
    "                pca.components_[y, i],  \n",
    "                head_width=0.07,\n",
    "                head_length=0.07, \n",
    "                width=0.02, )\n",
    "\n",
    "        # Les labels\n",
    "        plt.text(pca.components_[x, i] + 0.05,\n",
    "                pca.components_[y, i] + 0.05,\n",
    "                features[i])\n",
    "        \n",
    "    # Affichage des lignes horizontales et verticales\n",
    "    plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "    plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "    # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "    plt.xlabel('F{} ({}%)'.format(x+1, round(100*pca.explained_variance_ratio_[x],1)))\n",
    "    plt.ylabel('F{} ({}%)'.format(y+1, round(100*pca.explained_variance_ratio_[y],1)))\n",
    "\n",
    "    # Titre du graphique\n",
    "    plt.title(\"Cercle des corrélations (F{} et F{})\".format(x+1, y+1))\n",
    "\n",
    "    # Le cercle \n",
    "    an = np.linspace(0, 2 * np.pi, 100)\n",
    "    plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n",
    "\n",
    "    # Axes et display\n",
    "    plt.axis('equal')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "\n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    \n",
    "                                        # my_projection_individus\n",
    "        \n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    if projection == True:\n",
    "        # On projette nos données\n",
    "        X_projected = pca.transform(X_scaled)\n",
    "\n",
    "        # Transforme X_projected en np.array\n",
    "        X_ = np.array(X_projected)\n",
    "\n",
    "        # On définit la forme de la figure si elle n'a pas été donnée\n",
    "        if not figsize: \n",
    "            figsize = (7,6)\n",
    "\n",
    "        # On gère les labels\n",
    "        if  labels is None : \n",
    "            labels = []\n",
    "        try : \n",
    "            len(labels)\n",
    "        except Exception as e : \n",
    "            raise e\n",
    "\n",
    "        # On vérifie la variable axis \n",
    "        if not len(x_y) ==2 : \n",
    "            raise AttributeError(\"2 axes sont demandées\")   \n",
    "        if max(x_y )>= X_.shape[1] : \n",
    "            raise AttributeError(\"la variable axis n'est pas bonne\")   \n",
    "\n",
    "\n",
    "        # Initialisation de la figure       \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "        # On vérifie s'il y a des clusters ou non\n",
    "        cl = None if clusters is None else clusters\n",
    "\n",
    "                      \n",
    "\n",
    "\n",
    "        display(df)\n",
    "        display(center)\n",
    "        sns.scatterplot(data=None, x=X_[:, x], y=X_[:, y], hue=cl)\n",
    "        sns.scatterplot(center[:, x], center[:, y], c='red', marker='s', s='100000')\n",
    "\n",
    "\n",
    "\n",
    "        # Si la variable pca a été fournie, on peut calculer le % de variance de chaque axe \n",
    "        if pca : \n",
    "            v1 = str(round(100*pca.explained_variance_ratio_[x]))  + \" %\"\n",
    "            v2 = str(round(100*pca.explained_variance_ratio_[y]))  + \" %\"\n",
    "        else : \n",
    "            v1=v2= ''\n",
    "\n",
    "        # Nom des axes, avec le pourcentage d'inertie expliqué\n",
    "        ax.set_xlabel(f'F{x+1} {v1}')\n",
    "        ax.set_ylabel(f'F{y+1} {v2}')\n",
    "\n",
    "        # Valeur x max et y max\n",
    "        x_max = np.abs(X_[:, x]).max() *1.1\n",
    "        y_max = np.abs(X_[:, y]).max() *1.1\n",
    "\n",
    "        # On borne x et y \n",
    "        ax.set_xlim(left=-x_max, right=x_max)\n",
    "        ax.set_ylim(bottom= -y_max, top=y_max)\n",
    "\n",
    "        # Affichage des lignes horizontales et verticales\n",
    "        plt.plot([-x_max, x_max], [0, 0], color='grey', alpha=0.8)\n",
    "        plt.plot([0,0], [-y_max, y_max], color='grey', alpha=0.8)\n",
    "\n",
    "        # Affichage des labels des points\n",
    "        if len(labels) : \n",
    "            for i,(_x,_y) in enumerate(X_[:,[x,y]]):\n",
    "                plt.text(_x, _y+0.05, labels[i], fontsize='9', ha='center',va='center') \n",
    "\n",
    "        # Titre et display\n",
    "        plt.title(f\"Projection des individus (sur F{x+1} et F{y+1})\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863591f",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699afa0d",
   "metadata": {},
   "source": [
    "## my_backward_selected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f05deaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_backward_selected(data, response):\n",
    "    \"\"\"Linear model designed by backward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by backward selection\n",
    "           evaluated by parameters p-value\n",
    "    \"\"\"\n",
    "    remaining = set(data._get_numeric_data().columns)\n",
    "    if response in remaining:\n",
    "        remaining.remove(response)\n",
    "    cond = True\n",
    "\n",
    "    while remaining and cond:\n",
    "        formula = \"{} ~ {} + 1\".format(response,' + '.join(remaining))\n",
    "        print('_______________________________')\n",
    "        print(formula)\n",
    "        model = smf.ols(formula, data).fit()\n",
    "        score = model.pvalues[1:]\n",
    "        toRemove = score[score == score.max()]\n",
    "        if toRemove.values > 0.05:\n",
    "            print('remove', toRemove.index[0], '(p-value :', round(toRemove.values[0],3), ')')\n",
    "            remaining.remove(toRemove.index[0])\n",
    "        else:\n",
    "            cond = False\n",
    "            print('is the final model !')\n",
    "        print('')\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca8ddfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_backward_selected_logistic(data, response):\n",
    "    \"\"\"Logistic regression model designed by backward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels logistic regression model\n",
    "           with an intercept\n",
    "           selected by backward selection\n",
    "           evaluated by parameters p-value\n",
    "    \"\"\"\n",
    "    remaining = set(data._get_numeric_data().columns)\n",
    "    if response in remaining:\n",
    "        remaining.remove(response)\n",
    "    cond = True\n",
    "\n",
    "    while remaining and cond:\n",
    "        formula = \"{} ~ {} + 1\".format(response, ' + '.join(remaining))\n",
    "        print('_______________________________')\n",
    "        print(formula)\n",
    "        model = smf.logit(formula, data).fit()\n",
    "        score = model.pvalues[1:]\n",
    "        toRemove = score[score == score.max()]\n",
    "        if toRemove.values > 0.05:\n",
    "            print('remove', toRemove.index[0], '(p-value:', round(toRemove.values[0], 3), ')')\n",
    "            remaining.remove(toRemove.index[0])\n",
    "        else:\n",
    "            cond = False\n",
    "            print('is the final model!')\n",
    "        print('')\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c996733-171d-41d4-80b3-59460b0938de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Sommaire",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "638px",
    "left": "2491px",
    "top": "177px",
    "width": "322px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
